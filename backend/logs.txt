
==> Audit <==
|---------|-------------------------|----------|------|---------|---------------------|---------------------|
| Command |          Args           | Profile  | User | Version |     Start Time      |      End Time       |
|---------|-------------------------|----------|------|---------|---------------------|---------------------|
| start   |                         | minikube | me   | v1.35.0 | 15 May 25 01:32 MDT |                     |
| start   | --driver=docker         | minikube | me   | v1.35.0 | 15 May 25 01:37 MDT | 15 May 25 01:37 MDT |
| start   | --driver=docker         | minikube | me   | v1.35.0 | 15 May 25 01:40 MDT | 15 May 25 01:40 MDT |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 02:33 MDT |                     |
| image   | load fastapi-app:latest | minikube | me   | v1.35.0 | 15 May 25 02:43 MDT | 15 May 25 02:44 MDT |
| start   |                         | minikube | me   | v1.35.0 | 15 May 25 02:46 MDT | 15 May 25 02:46 MDT |
| image   | load fastapi-app:latest | minikube | me   | v1.35.0 | 15 May 25 02:48 MDT | 15 May 25 02:48 MDT |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 02:51 MDT |                     |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 02:52 MDT |                     |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 02:56 MDT |                     |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 03:06 MDT |                     |
| tunnel  |                         | minikube | me   | v1.35.0 | 15 May 25 03:14 MDT |                     |
| start   |                         | minikube | me   | v1.35.0 | 15 May 25 03:25 MDT | 15 May 25 03:25 MDT |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 03:26 MDT |                     |
| tunnel  |                         | minikube | me   | v1.35.0 | 15 May 25 03:27 MDT |                     |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 03:28 MDT |                     |
| image   | load fastapi-app:latest | minikube | me   | v1.35.0 | 15 May 25 03:40 MDT | 15 May 25 03:40 MDT |
| start   |                         | minikube | me   | v1.35.0 | 15 May 25 03:50 MDT | 15 May 25 03:51 MDT |
| start   |                         | minikube | me   | v1.35.0 | 15 May 25 03:51 MDT | 15 May 25 03:52 MDT |
| start   |                         | minikube | me   | v1.35.0 | 15 May 25 03:52 MDT | 15 May 25 03:53 MDT |
| start   |                         | minikube | me   | v1.35.0 | 15 May 25 04:14 MDT | 15 May 25 04:14 MDT |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 04:16 MDT |                     |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 04:19 MDT |                     |
| tunnel  |                         | minikube | me   | v1.35.0 | 15 May 25 04:19 MDT | 15 May 25 04:30 MDT |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 04:20 MDT |                     |
| start   |                         | minikube | me   | v1.35.0 | 15 May 25 04:21 MDT | 15 May 25 04:21 MDT |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 04:27 MDT |                     |
| delete  |                         | minikube | me   | v1.35.0 | 15 May 25 04:30 MDT | 15 May 25 04:30 MDT |
| start   | --driver=docker         | minikube | me   | v1.35.0 | 15 May 25 04:31 MDT | 15 May 25 04:31 MDT |
| service | fastapi-service --url   | minikube | me   | v1.35.0 | 15 May 25 04:43 MDT |                     |
|---------|-------------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/05/15 04:31:10
Running on machine: dev
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0515 04:31:10.086327   20974 out.go:345] Setting OutFile to fd 1 ...
I0515 04:31:10.086488   20974 out.go:397] isatty.IsTerminal(1) = true
I0515 04:31:10.086491   20974 out.go:358] Setting ErrFile to fd 2...
I0515 04:31:10.086493   20974 out.go:397] isatty.IsTerminal(2) = true
I0515 04:31:10.086583   20974 root.go:338] Updating PATH: /home/me/.minikube/bin
I0515 04:31:10.087105   20974 out.go:352] Setting JSON to false
I0515 04:31:10.088267   20974 start.go:129] hostinfo: {"hostname":"dev","uptime":1212,"bootTime":1747303858,"procs":410,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.8.0-59-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"bf199619-fc94-4a0a-aeea-1535f8bd4018"}
I0515 04:31:10.088297   20974 start.go:139] virtualization: kvm host
I0515 04:31:10.090986   20974 out.go:177] 😄  minikube v1.35.0 on Ubuntu 22.04
I0515 04:31:10.092462   20974 driver.go:394] Setting default libvirt URI to qemu:///system
I0515 04:31:10.092515   20974 notify.go:220] Checking for updates...
I0515 04:31:10.110111   20974 docker.go:123] docker version: linux-26.1.3:
I0515 04:31:10.110169   20974 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0515 04:31:10.125700   20974 info.go:266] docker info: {ID:56f33afb-852c-4551-be55-884cf35267f3 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:false NGoroutines:41 SystemTime:2025-05-15 04:31:10.11918961 -0600 MDT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-59-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:16418025472 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:dev Labels:[] ExperimentalBuild:false ServerVersion:26.1.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0515 04:31:10.125751   20974 docker.go:318] overlay module found
I0515 04:31:10.128068   20974 out.go:177] ✨  Using the docker driver based on user configuration
I0515 04:31:10.129310   20974 start.go:297] selected driver: docker
I0515 04:31:10.129314   20974 start.go:901] validating driver "docker" against <nil>
I0515 04:31:10.129322   20974 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0515 04:31:10.129380   20974 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0515 04:31:10.145709   20974 info.go:266] docker info: {ID:56f33afb-852c-4551-be55-884cf35267f3 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:false NGoroutines:41 SystemTime:2025-05-15 04:31:10.139933565 -0600 MDT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-59-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:16418025472 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:dev Labels:[] ExperimentalBuild:false ServerVersion:26.1.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0515 04:31:10.145797   20974 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0515 04:31:10.146560   20974 start_flags.go:393] Using suggested 3900MB memory alloc based on sys=15657MB, container=15657MB
I0515 04:31:10.146653   20974 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0515 04:31:10.148988   20974 out.go:177] 📌  Using Docker driver with root privileges
I0515 04:31:10.150224   20974 cni.go:84] Creating CNI manager for ""
I0515 04:31:10.150347   20974 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0515 04:31:10.150365   20974 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0515 04:31:10.150485   20974 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/me:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0515 04:31:10.152053   20974 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0515 04:31:10.153522   20974 cache.go:121] Beginning downloading kic base image for docker with docker
I0515 04:31:10.154323   20974 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0515 04:31:10.155534   20974 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0515 04:31:10.155551   20974 preload.go:146] Found local preload: /home/me/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0515 04:31:10.155555   20974 cache.go:56] Caching tarball of preloaded images
I0515 04:31:10.155598   20974 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0515 04:31:10.155629   20974 preload.go:172] Found /home/me/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0515 04:31:10.155635   20974 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0515 04:31:10.155962   20974 profile.go:143] Saving config to /home/me/.minikube/profiles/minikube/config.json ...
I0515 04:31:10.155971   20974 lock.go:35] WriteFile acquiring /home/me/.minikube/profiles/minikube/config.json: {Name:mk96e3f7c38e337ef347fca6742cdaecff43529f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 04:31:10.170437   20974 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0515 04:31:10.170445   20974 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0515 04:31:10.170452   20974 cache.go:227] Successfully downloaded all kic artifacts
I0515 04:31:10.170466   20974 start.go:360] acquireMachinesLock for minikube: {Name:mkdd194a767853f46d15047b5fcc66a09539b07a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0515 04:31:10.170495   20974 start.go:364] duration metric: took 19.844µs to acquireMachinesLock for "minikube"
I0515 04:31:10.170503   20974 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/me:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0515 04:31:10.170540   20974 start.go:125] createHost starting for "" (driver="docker")
I0515 04:31:10.172480   20974 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=3900MB) ...
I0515 04:31:10.172624   20974 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0515 04:31:10.172635   20974 client.go:168] LocalClient.Create starting
I0515 04:31:10.172690   20974 main.go:141] libmachine: Reading certificate data from /home/me/.minikube/certs/ca.pem
I0515 04:31:10.172704   20974 main.go:141] libmachine: Decoding PEM data...
I0515 04:31:10.172710   20974 main.go:141] libmachine: Parsing certificate...
I0515 04:31:10.172734   20974 main.go:141] libmachine: Reading certificate data from /home/me/.minikube/certs/cert.pem
I0515 04:31:10.172740   20974 main.go:141] libmachine: Decoding PEM data...
I0515 04:31:10.172744   20974 main.go:141] libmachine: Parsing certificate...
I0515 04:31:10.172928   20974 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0515 04:31:10.185339   20974 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0515 04:31:10.185387   20974 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0515 04:31:10.185399   20974 cli_runner.go:164] Run: docker network inspect minikube
W0515 04:31:10.198436   20974 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0515 04:31:10.198448   20974 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0515 04:31:10.198456   20974 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0515 04:31:10.198510   20974 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0515 04:31:10.209496   20974 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001921be0}
I0515 04:31:10.209518   20974 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0515 04:31:10.209538   20974 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0515 04:31:10.293746   20974 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0515 04:31:10.293782   20974 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0515 04:31:10.293903   20974 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0515 04:31:10.304722   20974 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0515 04:31:10.317591   20974 oci.go:103] Successfully created a docker volume minikube
I0515 04:31:10.317642   20974 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0515 04:31:10.691560   20974 oci.go:107] Successfully prepared a docker volume minikube
I0515 04:31:10.691575   20974 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0515 04:31:10.691587   20974 kic.go:194] Starting extracting preloaded images to volume ...
I0515 04:31:10.691624   20974 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/me/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0515 04:31:12.426580   20974 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/me/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (1.734935194s)
I0515 04:31:12.426592   20974 kic.go:203] duration metric: took 1.735003536s to extract preloaded images to volume ...
W0515 04:31:12.426642   20974 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0515 04:31:12.426657   20974 oci.go:249] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0515 04:31:12.426679   20974 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0515 04:31:12.439498   20974 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3900mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0515 04:31:12.664300   20974 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0515 04:31:12.674742   20974 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 04:31:12.683719   20974 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0515 04:31:12.720801   20974 oci.go:144] the created container "minikube" has a running status.
I0515 04:31:12.720814   20974 kic.go:225] Creating ssh key for kic: /home/me/.minikube/machines/minikube/id_rsa...
I0515 04:31:12.885165   20974 kic_runner.go:191] docker (temp): /home/me/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0515 04:31:12.903145   20974 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 04:31:12.914031   20974 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0515 04:31:12.914038   20974 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0515 04:31:12.961259   20974 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 04:31:12.970852   20974 machine.go:93] provisionDockerMachine start ...
I0515 04:31:12.970918   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:12.980159   20974 main.go:141] libmachine: Using SSH client type: native
I0515 04:31:12.980323   20974 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0515 04:31:12.980332   20974 main.go:141] libmachine: About to run SSH command:
hostname
I0515 04:31:13.097064   20974 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0515 04:31:13.097076   20974 ubuntu.go:169] provisioning hostname "minikube"
I0515 04:31:13.097127   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:13.108057   20974 main.go:141] libmachine: Using SSH client type: native
I0515 04:31:13.108191   20974 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0515 04:31:13.108196   20974 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0515 04:31:13.231184   20974 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0515 04:31:13.231227   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:13.240944   20974 main.go:141] libmachine: Using SSH client type: native
I0515 04:31:13.241051   20974 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0515 04:31:13.241059   20974 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0515 04:31:13.360849   20974 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0515 04:31:13.360867   20974 ubuntu.go:175] set auth options {CertDir:/home/me/.minikube CaCertPath:/home/me/.minikube/certs/ca.pem CaPrivateKeyPath:/home/me/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/me/.minikube/machines/server.pem ServerKeyPath:/home/me/.minikube/machines/server-key.pem ClientKeyPath:/home/me/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/me/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/me/.minikube}
I0515 04:31:13.360912   20974 ubuntu.go:177] setting up certificates
I0515 04:31:13.360919   20974 provision.go:84] configureAuth start
I0515 04:31:13.360981   20974 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0515 04:31:13.374590   20974 provision.go:143] copyHostCerts
I0515 04:31:13.374615   20974 exec_runner.go:144] found /home/me/.minikube/ca.pem, removing ...
I0515 04:31:13.374620   20974 exec_runner.go:203] rm: /home/me/.minikube/ca.pem
I0515 04:31:13.374666   20974 exec_runner.go:151] cp: /home/me/.minikube/certs/ca.pem --> /home/me/.minikube/ca.pem (1066 bytes)
I0515 04:31:13.374733   20974 exec_runner.go:144] found /home/me/.minikube/cert.pem, removing ...
I0515 04:31:13.374735   20974 exec_runner.go:203] rm: /home/me/.minikube/cert.pem
I0515 04:31:13.374746   20974 exec_runner.go:151] cp: /home/me/.minikube/certs/cert.pem --> /home/me/.minikube/cert.pem (1111 bytes)
I0515 04:31:13.374765   20974 exec_runner.go:144] found /home/me/.minikube/key.pem, removing ...
I0515 04:31:13.374766   20974 exec_runner.go:203] rm: /home/me/.minikube/key.pem
I0515 04:31:13.374777   20974 exec_runner.go:151] cp: /home/me/.minikube/certs/key.pem --> /home/me/.minikube/key.pem (1675 bytes)
I0515 04:31:13.374808   20974 provision.go:117] generating server cert: /home/me/.minikube/machines/server.pem ca-key=/home/me/.minikube/certs/ca.pem private-key=/home/me/.minikube/certs/ca-key.pem org=me.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0515 04:31:13.428742   20974 provision.go:177] copyRemoteCerts
I0515 04:31:13.428812   20974 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0515 04:31:13.428836   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:13.440008   20974 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/me/.minikube/machines/minikube/id_rsa Username:docker}
I0515 04:31:13.526625   20974 ssh_runner.go:362] scp /home/me/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1066 bytes)
I0515 04:31:13.545281   20974 ssh_runner.go:362] scp /home/me/.minikube/machines/server.pem --> /etc/docker/server.pem (1168 bytes)
I0515 04:31:13.561293   20974 ssh_runner.go:362] scp /home/me/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0515 04:31:13.579220   20974 provision.go:87] duration metric: took 218.291826ms to configureAuth
I0515 04:31:13.579233   20974 ubuntu.go:193] setting minikube options for container-runtime
I0515 04:31:13.579365   20974 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0515 04:31:13.579409   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:13.594294   20974 main.go:141] libmachine: Using SSH client type: native
I0515 04:31:13.594454   20974 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0515 04:31:13.594461   20974 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0515 04:31:13.718783   20974 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0515 04:31:13.718794   20974 ubuntu.go:71] root file system type: overlay
I0515 04:31:13.718857   20974 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0515 04:31:13.718910   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:13.736379   20974 main.go:141] libmachine: Using SSH client type: native
I0515 04:31:13.736528   20974 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0515 04:31:13.736594   20974 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0515 04:31:13.867865   20974 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0515 04:31:13.867932   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:13.883716   20974 main.go:141] libmachine: Using SSH client type: native
I0515 04:31:13.883877   20974 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0515 04:31:13.883902   20974 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0515 04:31:14.903773   20974 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-05-15 10:31:13.865124367 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0515 04:31:14.903795   20974 machine.go:96] duration metric: took 1.932928008s to provisionDockerMachine
I0515 04:31:14.903806   20974 client.go:171] duration metric: took 4.731166657s to LocalClient.Create
I0515 04:31:14.903822   20974 start.go:167] duration metric: took 4.731196813s to libmachine.API.Create "minikube"
I0515 04:31:14.903828   20974 start.go:293] postStartSetup for "minikube" (driver="docker")
I0515 04:31:14.903838   20974 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0515 04:31:14.903904   20974 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0515 04:31:14.903946   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:14.915477   20974 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/me/.minikube/machines/minikube/id_rsa Username:docker}
I0515 04:31:15.008224   20974 ssh_runner.go:195] Run: cat /etc/os-release
I0515 04:31:15.012847   20974 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0515 04:31:15.012872   20974 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0515 04:31:15.012883   20974 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0515 04:31:15.012908   20974 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0515 04:31:15.012917   20974 filesync.go:126] Scanning /home/me/.minikube/addons for local assets ...
I0515 04:31:15.012970   20974 filesync.go:126] Scanning /home/me/.minikube/files for local assets ...
I0515 04:31:15.012987   20974 start.go:296] duration metric: took 109.15475ms for postStartSetup
I0515 04:31:15.013299   20974 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0515 04:31:15.028705   20974 profile.go:143] Saving config to /home/me/.minikube/profiles/minikube/config.json ...
I0515 04:31:15.028842   20974 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0515 04:31:15.028862   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:15.041810   20974 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/me/.minikube/machines/minikube/id_rsa Username:docker}
I0515 04:31:15.132041   20974 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0515 04:31:15.137178   20974 start.go:128] duration metric: took 4.966625806s to createHost
I0515 04:31:15.137193   20974 start.go:83] releasing machines lock for "minikube", held for 4.966692799s
I0515 04:31:15.137250   20974 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0515 04:31:15.148779   20974 ssh_runner.go:195] Run: cat /version.json
I0515 04:31:15.148810   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:15.148920   20974 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0515 04:31:15.149002   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:15.159957   20974 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/me/.minikube/machines/minikube/id_rsa Username:docker}
I0515 04:31:15.161100   20974 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/me/.minikube/machines/minikube/id_rsa Username:docker}
I0515 04:31:15.242689   20974 ssh_runner.go:195] Run: systemctl --version
I0515 04:31:15.370719   20974 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0515 04:31:15.376849   20974 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0515 04:31:15.414311   20974 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0515 04:31:15.414382   20974 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0515 04:31:15.440417   20974 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0515 04:31:15.440432   20974 start.go:495] detecting cgroup driver to use...
I0515 04:31:15.440462   20974 detect.go:190] detected "systemd" cgroup driver on host os
I0515 04:31:15.440575   20974 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0515 04:31:15.458159   20974 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0515 04:31:15.470808   20974 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0515 04:31:15.482338   20974 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0515 04:31:15.482359   20974 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0515 04:31:15.490183   20974 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0515 04:31:15.499091   20974 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0515 04:31:15.505733   20974 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0515 04:31:15.512354   20974 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0515 04:31:15.518826   20974 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0515 04:31:15.525968   20974 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0515 04:31:15.532606   20974 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0515 04:31:15.538707   20974 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0515 04:31:15.543987   20974 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0515 04:31:15.549319   20974 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 04:31:15.586465   20974 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0515 04:31:15.657466   20974 start.go:495] detecting cgroup driver to use...
I0515 04:31:15.657495   20974 detect.go:190] detected "systemd" cgroup driver on host os
I0515 04:31:15.657535   20974 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0515 04:31:15.669333   20974 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0515 04:31:15.669362   20974 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0515 04:31:15.676053   20974 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0515 04:31:15.688937   20974 ssh_runner.go:195] Run: which cri-dockerd
I0515 04:31:15.691210   20974 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0515 04:31:15.697333   20974 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0515 04:31:15.710625   20974 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0515 04:31:15.754912   20974 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0515 04:31:15.792843   20974 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0515 04:31:15.792935   20974 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0515 04:31:15.805100   20974 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 04:31:15.843856   20974 ssh_runner.go:195] Run: sudo systemctl restart docker
I0515 04:31:17.264996   20974 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.421120454s)
I0515 04:31:17.265033   20974 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0515 04:31:17.278070   20974 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0515 04:31:17.287513   20974 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0515 04:31:17.340714   20974 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0515 04:31:17.376807   20974 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 04:31:17.410450   20974 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0515 04:31:17.430581   20974 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0515 04:31:17.437102   20974 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 04:31:17.469518   20974 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0515 04:31:17.544060   20974 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0515 04:31:17.544100   20974 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0515 04:31:17.547871   20974 start.go:563] Will wait 60s for crictl version
I0515 04:31:17.547915   20974 ssh_runner.go:195] Run: which crictl
I0515 04:31:17.551544   20974 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0515 04:31:17.577655   20974 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0515 04:31:17.577693   20974 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0515 04:31:17.592094   20974 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0515 04:31:17.608818   20974 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0515 04:31:17.608903   20974 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0515 04:31:17.619895   20974 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0515 04:31:17.623354   20974 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0515 04:31:17.633553   20974 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/me:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0515 04:31:17.633626   20974 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0515 04:31:17.633662   20974 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0515 04:31:17.647234   20974 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0515 04:31:17.647242   20974 docker.go:619] Images already preloaded, skipping extraction
I0515 04:31:17.647280   20974 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0515 04:31:17.659687   20974 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0515 04:31:17.659696   20974 cache_images.go:84] Images are preloaded, skipping loading
I0515 04:31:17.659702   20974 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0515 04:31:17.659760   20974 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0515 04:31:17.659791   20974 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0515 04:31:17.696724   20974 cni.go:84] Creating CNI manager for ""
I0515 04:31:17.696734   20974 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0515 04:31:17.696739   20974 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0515 04:31:17.696749   20974 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0515 04:31:17.696813   20974 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0515 04:31:17.696844   20974 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0515 04:31:17.705447   20974 binaries.go:44] Found k8s binaries, skipping transfer
I0515 04:31:17.705492   20974 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0515 04:31:17.713549   20974 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0515 04:31:17.731432   20974 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0515 04:31:17.747866   20974 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0515 04:31:17.762513   20974 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0515 04:31:17.766490   20974 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0515 04:31:17.778260   20974 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 04:31:17.822212   20974 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0515 04:31:17.844566   20974 certs.go:68] Setting up /home/me/.minikube/profiles/minikube for IP: 192.168.49.2
I0515 04:31:17.844573   20974 certs.go:194] generating shared ca certs ...
I0515 04:31:17.844582   20974 certs.go:226] acquiring lock for ca certs: {Name:mk19247bd49d0bc1645bf3e3bfb27fefadd80266 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 04:31:17.844653   20974 certs.go:235] skipping valid "minikubeCA" ca cert: /home/me/.minikube/ca.key
I0515 04:31:17.844669   20974 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/me/.minikube/proxy-client-ca.key
I0515 04:31:17.844673   20974 certs.go:256] generating profile certs ...
I0515 04:31:17.844701   20974 certs.go:363] generating signed profile cert for "minikube-user": /home/me/.minikube/profiles/minikube/client.key
I0515 04:31:17.844705   20974 crypto.go:68] Generating cert /home/me/.minikube/profiles/minikube/client.crt with IP's: []
I0515 04:31:17.906539   20974 crypto.go:156] Writing cert to /home/me/.minikube/profiles/minikube/client.crt ...
I0515 04:31:17.906547   20974 lock.go:35] WriteFile acquiring /home/me/.minikube/profiles/minikube/client.crt: {Name:mk1c3ece67dbc6ada6710b94632fd98e465b0d43 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 04:31:17.906655   20974 crypto.go:164] Writing key to /home/me/.minikube/profiles/minikube/client.key ...
I0515 04:31:17.906658   20974 lock.go:35] WriteFile acquiring /home/me/.minikube/profiles/minikube/client.key: {Name:mk6b88fe71b2c3ea215f31bdcb255bc438d98d10 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 04:31:17.906690   20974 certs.go:363] generating signed profile cert for "minikube": /home/me/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0515 04:31:17.906695   20974 crypto.go:68] Generating cert /home/me/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0515 04:31:17.955369   20974 crypto.go:156] Writing cert to /home/me/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0515 04:31:17.955374   20974 lock.go:35] WriteFile acquiring /home/me/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk24109ff663aeaeeffbc289b394e65116b50019 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 04:31:17.955461   20974 crypto.go:164] Writing key to /home/me/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0515 04:31:17.955463   20974 lock.go:35] WriteFile acquiring /home/me/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mkd56e7a64342a45dda973d8bafa3026dd0f37b9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 04:31:17.955490   20974 certs.go:381] copying /home/me/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/me/.minikube/profiles/minikube/apiserver.crt
I0515 04:31:17.955524   20974 certs.go:385] copying /home/me/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/me/.minikube/profiles/minikube/apiserver.key
I0515 04:31:17.955543   20974 certs.go:363] generating signed profile cert for "aggregator": /home/me/.minikube/profiles/minikube/proxy-client.key
I0515 04:31:17.955548   20974 crypto.go:68] Generating cert /home/me/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0515 04:31:18.034632   20974 crypto.go:156] Writing cert to /home/me/.minikube/profiles/minikube/proxy-client.crt ...
I0515 04:31:18.034637   20974 lock.go:35] WriteFile acquiring /home/me/.minikube/profiles/minikube/proxy-client.crt: {Name:mk89c473cfe24225ae14e8dbf6b4388b6c9d60b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 04:31:18.034714   20974 crypto.go:164] Writing key to /home/me/.minikube/profiles/minikube/proxy-client.key ...
I0515 04:31:18.034717   20974 lock.go:35] WriteFile acquiring /home/me/.minikube/profiles/minikube/proxy-client.key: {Name:mk09083b0ff0b175024c3cb6bb79c41b2e237742 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 04:31:18.034782   20974 certs.go:484] found cert: /home/me/.minikube/certs/ca-key.pem (1679 bytes)
I0515 04:31:18.034793   20974 certs.go:484] found cert: /home/me/.minikube/certs/ca.pem (1066 bytes)
I0515 04:31:18.034801   20974 certs.go:484] found cert: /home/me/.minikube/certs/cert.pem (1111 bytes)
I0515 04:31:18.034808   20974 certs.go:484] found cert: /home/me/.minikube/certs/key.pem (1675 bytes)
I0515 04:31:18.035084   20974 ssh_runner.go:362] scp /home/me/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0515 04:31:18.054387   20974 ssh_runner.go:362] scp /home/me/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0515 04:31:18.071319   20974 ssh_runner.go:362] scp /home/me/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0515 04:31:18.091545   20974 ssh_runner.go:362] scp /home/me/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0515 04:31:18.111297   20974 ssh_runner.go:362] scp /home/me/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0515 04:31:18.126293   20974 ssh_runner.go:362] scp /home/me/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0515 04:31:18.144294   20974 ssh_runner.go:362] scp /home/me/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0515 04:31:18.162422   20974 ssh_runner.go:362] scp /home/me/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0515 04:31:18.179460   20974 ssh_runner.go:362] scp /home/me/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0515 04:31:18.208743   20974 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0515 04:31:18.227000   20974 ssh_runner.go:195] Run: openssl version
I0515 04:31:18.231914   20974 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0515 04:31:18.241569   20974 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0515 04:31:18.243863   20974 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 15 07:37 /usr/share/ca-certificates/minikubeCA.pem
I0515 04:31:18.243888   20974 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0515 04:31:18.247982   20974 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0515 04:31:18.254012   20974 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0515 04:31:18.256279   20974 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0515 04:31:18.256304   20974 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/me:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0515 04:31:18.256366   20974 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0515 04:31:18.269958   20974 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0515 04:31:18.276284   20974 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0515 04:31:18.283156   20974 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0515 04:31:18.283178   20974 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0515 04:31:18.287885   20974 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0515 04:31:18.287894   20974 kubeadm.go:157] found existing configuration files:

I0515 04:31:18.287924   20974 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0515 04:31:18.294750   20974 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0515 04:31:18.294792   20974 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0515 04:31:18.300346   20974 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0515 04:31:18.307533   20974 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0515 04:31:18.307555   20974 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0515 04:31:18.315240   20974 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0515 04:31:18.321497   20974 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0515 04:31:18.321524   20974 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0515 04:31:18.327786   20974 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0515 04:31:18.333577   20974 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0515 04:31:18.333631   20974 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0515 04:31:18.339227   20974 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0515 04:31:18.364296   20974 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0515 04:31:18.364322   20974 kubeadm.go:310] [preflight] Running pre-flight checks
I0515 04:31:18.377109   20974 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I0515 04:31:18.377148   20974 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.8.0-59-generic[0m
I0515 04:31:18.377169   20974 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I0515 04:31:18.377195   20974 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0515 04:31:18.377219   20974 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0515 04:31:18.377241   20974 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0515 04:31:18.377264   20974 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0515 04:31:18.377286   20974 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0515 04:31:18.377309   20974 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0515 04:31:18.377331   20974 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0515 04:31:18.377350   20974 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I0515 04:31:18.406815   20974 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0515 04:31:18.406872   20974 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0515 04:31:18.406927   20974 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0515 04:31:18.412119   20974 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0515 04:31:18.413435   20974 out.go:235]     ▪ Generating certificates and keys ...
I0515 04:31:18.413503   20974 kubeadm.go:310] [certs] Using existing ca certificate authority
I0515 04:31:18.413546   20974 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0515 04:31:18.510379   20974 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0515 04:31:18.554885   20974 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0515 04:31:18.628450   20974 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0515 04:31:18.788038   20974 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0515 04:31:18.849809   20974 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0515 04:31:18.849894   20974 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0515 04:31:18.891806   20974 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0515 04:31:18.891897   20974 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0515 04:31:18.997723   20974 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0515 04:31:19.087174   20974 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0515 04:31:19.126944   20974 kubeadm.go:310] [certs] Generating "sa" key and public key
I0515 04:31:19.127007   20974 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0515 04:31:19.229222   20974 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0515 04:31:19.351426   20974 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0515 04:31:19.451122   20974 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0515 04:31:19.543893   20974 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0515 04:31:19.707456   20974 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0515 04:31:19.707533   20974 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0515 04:31:19.714787   20974 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0515 04:31:19.715820   20974 out.go:235]     ▪ Booting up control plane ...
I0515 04:31:19.715907   20974 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0515 04:31:19.715947   20974 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0515 04:31:19.715994   20974 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0515 04:31:19.721579   20974 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0515 04:31:19.725091   20974 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0515 04:31:19.725115   20974 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0515 04:31:19.778821   20974 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0515 04:31:19.778947   20974 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0515 04:31:20.281120   20974 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 502.242981ms
I0515 04:31:20.281266   20974 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0515 04:31:22.782741   20974 kubeadm.go:310] [api-check] The API server is healthy after 2.501626335s
I0515 04:31:22.796428   20974 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0515 04:31:22.803922   20974 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0515 04:31:22.818640   20974 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0515 04:31:22.818868   20974 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0515 04:31:22.824885   20974 kubeadm.go:310] [bootstrap-token] Using token: e01vbv.avrw0o3lk2oicy40
I0515 04:31:22.825575   20974 out.go:235]     ▪ Configuring RBAC rules ...
I0515 04:31:22.825778   20974 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0515 04:31:22.829220   20974 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0515 04:31:22.834267   20974 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0515 04:31:22.837082   20974 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0515 04:31:22.839310   20974 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0515 04:31:22.841438   20974 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0515 04:31:23.195696   20974 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0515 04:31:23.601882   20974 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0515 04:31:24.194063   20974 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0515 04:31:24.195170   20974 kubeadm.go:310] 
I0515 04:31:24.195262   20974 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0515 04:31:24.195269   20974 kubeadm.go:310] 
I0515 04:31:24.195376   20974 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0515 04:31:24.195381   20974 kubeadm.go:310] 
I0515 04:31:24.195413   20974 kubeadm.go:310]   mkdir -p $HOME/.kube
I0515 04:31:24.195511   20974 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0515 04:31:24.195581   20974 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0515 04:31:24.195587   20974 kubeadm.go:310] 
I0515 04:31:24.195678   20974 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0515 04:31:24.195688   20974 kubeadm.go:310] 
I0515 04:31:24.195753   20974 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0515 04:31:24.195759   20974 kubeadm.go:310] 
I0515 04:31:24.195826   20974 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0515 04:31:24.195951   20974 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0515 04:31:24.196043   20974 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0515 04:31:24.196048   20974 kubeadm.go:310] 
I0515 04:31:24.196164   20974 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0515 04:31:24.196287   20974 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0515 04:31:24.196297   20974 kubeadm.go:310] 
I0515 04:31:24.196410   20974 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token e01vbv.avrw0o3lk2oicy40 \
I0515 04:31:24.196559   20974 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:1011ddf2f478f51c5023d9dcd524b5519c8641fc46dbf3768551cfcf0502dbaa \
I0515 04:31:24.196589   20974 kubeadm.go:310] 	--control-plane 
I0515 04:31:24.196595   20974 kubeadm.go:310] 
I0515 04:31:24.196710   20974 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0515 04:31:24.196715   20974 kubeadm.go:310] 
I0515 04:31:24.196826   20974 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token e01vbv.avrw0o3lk2oicy40 \
I0515 04:31:24.196974   20974 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:1011ddf2f478f51c5023d9dcd524b5519c8641fc46dbf3768551cfcf0502dbaa 
I0515 04:31:24.199253   20974 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0515 04:31:24.199513   20974 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.8.0-59-generic\n", err: exit status 1
I0515 04:31:24.199654   20974 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0515 04:31:24.199672   20974 cni.go:84] Creating CNI manager for ""
I0515 04:31:24.199686   20974 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0515 04:31:24.200521   20974 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0515 04:31:24.201148   20974 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0515 04:31:24.211504   20974 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0515 04:31:24.231524   20974 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0515 04:31:24.231632   20974 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0515 04:31:24.231652   20974 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_05_15T04_31_24_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0515 04:31:24.240057   20974 ops.go:34] apiserver oom_adj: -16
I0515 04:31:24.283147   20974 kubeadm.go:1113] duration metric: took 51.569386ms to wait for elevateKubeSystemPrivileges
I0515 04:31:24.283163   20974 kubeadm.go:394] duration metric: took 6.026860713s to StartCluster
I0515 04:31:24.283178   20974 settings.go:142] acquiring lock: {Name:mk14a2280a6b8ec984f7279cd585ccf0881240f6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 04:31:24.283232   20974 settings.go:150] Updating kubeconfig:  /home/me/.kube/config
I0515 04:31:24.283759   20974 lock.go:35] WriteFile acquiring /home/me/.kube/config: {Name:mk40aea569c5cc1240d02dc67cf4e21e39bd0d71 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 04:31:24.283936   20974 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0515 04:31:24.283984   20974 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0515 04:31:24.283996   20974 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0515 04:31:24.284059   20974 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0515 04:31:24.284064   20974 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0515 04:31:24.284073   20974 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0515 04:31:24.284078   20974 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0515 04:31:24.284096   20974 host.go:66] Checking if "minikube" exists ...
I0515 04:31:24.284134   20974 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0515 04:31:24.284304   20974 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 04:31:24.284437   20974 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 04:31:24.284510   20974 out.go:177] 🔎  Verifying Kubernetes components...
I0515 04:31:24.287791   20974 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 04:31:24.298087   20974 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0515 04:31:24.298568   20974 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0515 04:31:24.298599   20974 host.go:66] Checking if "minikube" exists ...
I0515 04:31:24.298713   20974 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0515 04:31:24.298720   20974 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0515 04:31:24.298771   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:24.298996   20974 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 04:31:24.311431   20974 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0515 04:31:24.311449   20974 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0515 04:31:24.311523   20974 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 04:31:24.312341   20974 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/me/.minikube/machines/minikube/id_rsa Username:docker}
I0515 04:31:24.326368   20974 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/me/.minikube/machines/minikube/id_rsa Username:docker}
I0515 04:31:24.342653   20974 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0515 04:31:24.348386   20974 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0515 04:31:24.401760   20974 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0515 04:31:24.415237   20974 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0515 04:31:24.425067   20974 start.go:971] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0515 04:31:24.425695   20974 api_server.go:52] waiting for apiserver process to appear ...
I0515 04:31:24.425731   20974 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0515 04:31:24.542282   20974 api_server.go:72] duration metric: took 258.327716ms to wait for apiserver process to appear ...
I0515 04:31:24.542290   20974 api_server.go:88] waiting for apiserver healthz status ...
I0515 04:31:24.542300   20974 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0515 04:31:24.545187   20974 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0515 04:31:24.545560   20974 api_server.go:141] control plane version: v1.32.0
I0515 04:31:24.545575   20974 api_server.go:131] duration metric: took 3.277272ms to wait for apiserver health ...
I0515 04:31:24.545580   20974 system_pods.go:43] waiting for kube-system pods to appear ...
I0515 04:31:24.545961   20974 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0515 04:31:24.546916   20974 addons.go:514] duration metric: took 262.921283ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0515 04:31:24.548686   20974 system_pods.go:59] 5 kube-system pods found
I0515 04:31:24.548694   20974 system_pods.go:61] "etcd-minikube" [95e44ab1-58cd-48c5-9bb5-c5e5428e1074] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0515 04:31:24.548698   20974 system_pods.go:61] "kube-apiserver-minikube" [e1729517-5c5d-404a-a078-e67cb9bf4a26] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0515 04:31:24.548703   20974 system_pods.go:61] "kube-controller-manager-minikube" [c64958ad-e6df-41e7-ac8e-394ba5b4f48e] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0515 04:31:24.548705   20974 system_pods.go:61] "kube-scheduler-minikube" [c2fa9262-9e19-4d6b-ab96-ed7bdd30b82c] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0515 04:31:24.548708   20974 system_pods.go:61] "storage-provisioner" [64cb1502-33c4-4360-8481-39131d2783ec] Pending
I0515 04:31:24.548710   20974 system_pods.go:74] duration metric: took 3.127466ms to wait for pod list to return data ...
I0515 04:31:24.548715   20974 kubeadm.go:582] duration metric: took 264.763637ms to wait for: map[apiserver:true system_pods:true]
I0515 04:31:24.548721   20974 node_conditions.go:102] verifying NodePressure condition ...
I0515 04:31:24.550059   20974 node_conditions.go:122] node storage ephemeral capacity is 102626232Ki
I0515 04:31:24.550066   20974 node_conditions.go:123] node cpu capacity is 20
I0515 04:31:24.550075   20974 node_conditions.go:105] duration metric: took 1.349664ms to run NodePressure ...
I0515 04:31:24.550080   20974 start.go:241] waiting for startup goroutines ...
I0515 04:31:24.929401   20974 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0515 04:31:24.929430   20974 start.go:246] waiting for cluster config update ...
I0515 04:31:24.929445   20974 start.go:255] writing updated cluster config ...
I0515 04:31:24.929763   20974 ssh_runner.go:195] Run: rm -f paused
I0515 04:31:24.965182   20974 start.go:600] kubectl: 1.33.0, cluster: 1.32.0 (minor skew: 1)
I0515 04:31:24.966156   20974 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 15 10:31:16 minikube dockerd[1289]: time="2025-05-15T10:31:16.514238381Z" level=info msg="Loading containers: start."
May 15 10:31:17 minikube dockerd[1289]: time="2025-05-15T10:31:17.068024549Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
May 15 10:31:17 minikube dockerd[1289]: time="2025-05-15T10:31:17.213735767Z" level=info msg="Loading containers: done."
May 15 10:31:17 minikube dockerd[1289]: time="2025-05-15T10:31:17.231468974Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
May 15 10:31:17 minikube dockerd[1289]: time="2025-05-15T10:31:17.231522097Z" level=info msg="Daemon has completed initialization"
May 15 10:31:17 minikube dockerd[1289]: time="2025-05-15T10:31:17.263197181Z" level=info msg="API listen on /var/run/docker.sock"
May 15 10:31:17 minikube dockerd[1289]: time="2025-05-15T10:31:17.263241349Z" level=info msg="API listen on [::]:2376"
May 15 10:31:17 minikube systemd[1]: Started Docker Application Container Engine.
May 15 10:31:17 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 15 10:31:17 minikube cri-dockerd[1567]: time="2025-05-15T10:31:17Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 15 10:31:17 minikube cri-dockerd[1567]: time="2025-05-15T10:31:17Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 15 10:31:17 minikube cri-dockerd[1567]: time="2025-05-15T10:31:17Z" level=info msg="Start docker client with request timeout 0s"
May 15 10:31:17 minikube cri-dockerd[1567]: time="2025-05-15T10:31:17Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 15 10:31:17 minikube cri-dockerd[1567]: time="2025-05-15T10:31:17Z" level=info msg="Loaded network plugin cni"
May 15 10:31:17 minikube cri-dockerd[1567]: time="2025-05-15T10:31:17Z" level=info msg="Docker cri networking managed by network plugin cni"
May 15 10:31:17 minikube cri-dockerd[1567]: time="2025-05-15T10:31:17Z" level=info msg="Setting cgroupDriver systemd"
May 15 10:31:17 minikube cri-dockerd[1567]: time="2025-05-15T10:31:17Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 15 10:31:17 minikube cri-dockerd[1567]: time="2025-05-15T10:31:17Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 15 10:31:17 minikube cri-dockerd[1567]: time="2025-05-15T10:31:17Z" level=info msg="Start cri-dockerd grpc backend"
May 15 10:31:17 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 15 10:31:20 minikube cri-dockerd[1567]: time="2025-05-15T10:31:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d13c0ef300a7a86bb3e7a1f0bd053ede893077419415a6f8700c03867d689078/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
May 15 10:31:20 minikube cri-dockerd[1567]: time="2025-05-15T10:31:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/784ce4346e67dc8045b3478cfbd4ec6019b1c4f4445f28c77e30c8a99508c7e2/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 15 10:31:20 minikube cri-dockerd[1567]: time="2025-05-15T10:31:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8ef3fc315c99270020a9787a979fb980f48e5e991b475ca77e2f74d0ed1bb1c9/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 15 10:31:20 minikube cri-dockerd[1567]: time="2025-05-15T10:31:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/34d7e62530730b18ba1da037c3003def3014424a307c2a4782b8402b4320964c/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 15 10:31:29 minikube cri-dockerd[1567]: time="2025-05-15T10:31:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d29e438db590e5bd3c387d99522506999741366f2432ee36bf2a9d5fc8b984fd/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 15 10:31:29 minikube cri-dockerd[1567]: time="2025-05-15T10:31:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7e40cafe2e8b5f4bab5c6c33726b19ed653e4f93891863917c552a7dd2e30841/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 15 10:31:29 minikube cri-dockerd[1567]: time="2025-05-15T10:31:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ca78041e373dee6840488535bc574838f7341f24feaf2fe3714ce418c5a7494f/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
May 15 10:31:33 minikube cri-dockerd[1567]: time="2025-05-15T10:31:33Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 15 10:31:43 minikube cri-dockerd[1567]: time="2025-05-15T10:31:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/36e70f1e13a9f87082e917c7105fa3efaa91918f96cc2df86f42886c7258451e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 15 10:31:43 minikube cri-dockerd[1567]: time="2025-05-15T10:31:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dbfab2b89d1712f630ac33ae5d768d5d191dd4af0b9594c51d32f2967774cd4e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 15 10:31:44 minikube dockerd[1289]: time="2025-05-15T10:31:44.102577316Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:31:44 minikube dockerd[1289]: time="2025-05-15T10:31:44.102635560Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:31:44 minikube dockerd[1289]: time="2025-05-15T10:31:44.827343066Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:31:44 minikube dockerd[1289]: time="2025-05-15T10:31:44.827360363Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:31:58 minikube dockerd[1289]: time="2025-05-15T10:31:58.104743404Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:31:58 minikube dockerd[1289]: time="2025-05-15T10:31:58.104818995Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:32:00 minikube dockerd[1289]: time="2025-05-15T10:32:00.069276150Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:32:00 minikube dockerd[1289]: time="2025-05-15T10:32:00.069311512Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:32:25 minikube dockerd[1289]: time="2025-05-15T10:32:25.123605121Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:32:25 minikube dockerd[1289]: time="2025-05-15T10:32:25.123661435Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:32:29 minikube dockerd[1289]: time="2025-05-15T10:32:29.062885344Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:32:29 minikube dockerd[1289]: time="2025-05-15T10:32:29.062935584Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:33:16 minikube dockerd[1289]: time="2025-05-15T10:33:16.128479332Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:33:16 minikube dockerd[1289]: time="2025-05-15T10:33:16.128519279Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:33:17 minikube dockerd[1289]: time="2025-05-15T10:33:17.042548120Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:33:17 minikube dockerd[1289]: time="2025-05-15T10:33:17.042565282Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:34:38 minikube dockerd[1289]: time="2025-05-15T10:34:38.126662910Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:34:38 minikube dockerd[1289]: time="2025-05-15T10:34:38.126685889Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:34:46 minikube dockerd[1289]: time="2025-05-15T10:34:46.089051103Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:34:46 minikube dockerd[1289]: time="2025-05-15T10:34:46.089090182Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:37:21 minikube dockerd[1289]: time="2025-05-15T10:37:21.114238468Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:37:21 minikube dockerd[1289]: time="2025-05-15T10:37:21.114296001Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:37:34 minikube dockerd[1289]: time="2025-05-15T10:37:34.082868930Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:37:34 minikube dockerd[1289]: time="2025-05-15T10:37:34.082923029Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:42:26 minikube dockerd[1289]: time="2025-05-15T10:42:26.136420323Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:42:26 minikube dockerd[1289]: time="2025-05-15T10:42:26.136464711Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 15 10:42:35 minikube cri-dockerd[1567]: time="2025-05-15T10:42:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/79728eddb249f1a7ce978f40e1bed1c641e027dbcd0f78ca389fd1c54e78aeb3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 15 10:42:37 minikube cri-dockerd[1567]: time="2025-05-15T10:42:37Z" level=info msg="Stop pulling image redis:7-alpine: Status: Downloaded newer image for redis:7-alpine"
May 15 10:42:46 minikube dockerd[1289]: time="2025-05-15T10:42:46.084496390Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 15 10:42:46 minikube dockerd[1289]: time="2025-05-15T10:42:46.084565360Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
70e830dddd1cb       redis@sha256:f773b35a95e170d92dd4214a3ec4859b1b7960bf56896ae687646d695f311187   2 minutes ago       Running             redis                     0                   79728eddb249f       redis-698496554f-8f62x
9d8d92c8b6e36       6e38f40d628db                                                                   13 minutes ago      Running             storage-provisioner       0                   ca78041e373de       storage-provisioner
ccb0cf314a02d       c69fa2e9cbf5f                                                                   13 minutes ago      Running             coredns                   0                   7e40cafe2e8b5       coredns-668d6bf9bc-hdngh
34693989e8b6e       040f9f8aac8cd                                                                   13 minutes ago      Running             kube-proxy                0                   d29e438db590e       kube-proxy-4npbp
e4546d100eb42       a389e107f4ff1                                                                   13 minutes ago      Running             kube-scheduler            0                   34d7e62530730       kube-scheduler-minikube
ea69923b8dfad       8cab3d2a8bd0f                                                                   13 minutes ago      Running             kube-controller-manager   0                   8ef3fc315c992       kube-controller-manager-minikube
988de64cbc186       a9e7e6b294baf                                                                   13 minutes ago      Running             etcd                      0                   d13c0ef300a7a       etcd-minikube
b5f11c86b0078       c2e17b8d0f4a3                                                                   13 minutes ago      Running             kube-apiserver            0                   784ce4346e67d       kube-apiserver-minikube


==> coredns [ccb0cf314a02] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:43615 - 26703 "HINFO IN 1884092188004104112.3243581225680226941. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.085065452s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1681781625]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (15-May-2025 10:31:29.642) (total time: 30000ms):
Trace[1681781625]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (10:31:59.642)
Trace[1681781625]: [30.000681029s] [30.000681029s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[958395855]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (15-May-2025 10:31:29.642) (total time: 30000ms):
Trace[958395855]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (10:31:59.642)
Trace[958395855]: [30.00084927s] [30.00084927s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[624157502]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (15-May-2025 10:31:29.642) (total time: 30000ms):
Trace[624157502]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (10:31:59.642)
Trace[624157502]: [30.000941511s] [30.000941511s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_15T04_31_24_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 15 May 2025 10:31:21 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 15 May 2025 10:44:41 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 15 May 2025 10:42:57 +0000   Thu, 15 May 2025 10:31:20 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 15 May 2025 10:42:57 +0000   Thu, 15 May 2025 10:31:20 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 15 May 2025 10:42:57 +0000   Thu, 15 May 2025 10:31:20 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 15 May 2025 10:42:57 +0000   Thu, 15 May 2025 10:31:21 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                20
  ephemeral-storage:  102626232Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16033228Ki
  pods:               110
Allocatable:
  cpu:                20
  ephemeral-storage:  102626232Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16033228Ki
  pods:               110
System Info:
  Machine ID:                 cb31057208ab44b089b15eb179c341f3
  System UUID:                04037272-a0bc-49ea-8097-5917ba55aabf
  Boot ID:                    063dd3f4-7e9c-483e-a44e-00b7475d764a
  Kernel Version:             6.8.0-59-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     fastapi-app-d94d949fd-5q2q6         0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m
  default                     fastapi-app-d94d949fd-cc4w8         0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m
  default                     redis-698496554f-8f62x              0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m7s
  kube-system                 coredns-668d6bf9bc-hdngh            100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     13m
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         13m
  kube-system                 kube-apiserver-minikube             250m (1%)     0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 kube-proxy-4npbp                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (3%)   0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age   From             Message
  ----    ------                   ----  ----             -------
  Normal  Starting                 13m   kube-proxy       
  Normal  Starting                 13m   kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  13m   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  13m   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    13m   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     13m   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           13m   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.012000] i8042: PNP: PS/2 appears to have AUX port disabled, if this is incorrect please boot with i8042.nopnp
[  +0.009589] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000103] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000000] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.159688] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'
[  +0.020523] blacklist: Duplicate blacklisted hash bin:47ff1b63b140b6fc04ed79131331e651da5b2e2f170f5daef4153dc2fbc532b1
[  +0.000001] blacklist: Duplicate blacklisted hash bin:5391c3a2fb112102a6aa1edc25ae77e19f5d6f09cd09eeb2509922bfcd5992ea
[  +0.000021] blacklist: Duplicate blacklisted hash bin:80b4d96931bf0d02fd91a61e19d14f1da452e66db2408ca8604d411f92659f0a
[  +0.000016] blacklist: Duplicate blacklisted hash bin:992d359aa7a5f789d268b94c11b9485a6b1ce64362b0edb4441ccc187c39647b
[  +0.000022] blacklist: Duplicate blacklisted hash bin:c452ab846073df5ace25cca64d6b7a09d906308a1a65eb5240e3c4ebcaa9cc0c
[  +0.000012] blacklist: Duplicate blacklisted hash bin:e051b788ecbaeda53046c70e6af6058f95222c046157b8c4c1b9c2cfc65f46e5
[  +0.126797] r8169 0000:2f:00.0: can't disable ASPM; OS doesn't have ASPM control
[  +0.404772] block nvme0n1: the capability attribute has been deprecated.
[  +0.019982] systemd[1]: Configuration file /run/systemd/system/netplan-ovs-cleanup.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
[  +0.007374] systemd[1]: /lib/systemd/system/snapd.service:23: Unknown key name 'RestartMode' in section 'Service', ignoring.
[May15 10:11] resource: resource sanity check: requesting [mem 0x00000000fedc0000-0x00000000fedcffff], which spans more than pnp 00:03 [mem 0xfedc0000-0xfedc7fff]
[  +0.000008] caller igen6_probe+0x1bb/0x5f0 [igen6_edac] mapping multiple BARs
[  +0.008482] proc_thermal_pci 0000:00:04.0: error: proc_thermal_add, will continue
[  +0.001766] ACPI Warning: \_SB.PC00.XHCI.RHUB.HS10._DSM: Argument #4 type mismatch - Found [Integer], ACPI requires [Package] (20230628/nsarguments-61)
[  +0.120936] nvidia: loading out-of-tree module taints kernel.
[  +0.000008] nvidia: module license 'NVIDIA' taints kernel.
[  +0.000000] Disabling lock debugging due to kernel taint
[  +0.000002] nvidia: module license taints kernel.

[  +0.261109] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  470.256.02  Thu May  2 14:37:44 UTC 2024
[  +0.116348] thermal thermal_zone2: failed to read out thermal zone (-61)
[  +0.091103] ACPI Warning: \_SB.PC00.PEG1.PEGP._DSM: Argument #4 type mismatch - Found [Buffer], ACPI requires [Package] (20230628/nsarguments-61)
[  +1.173405] nvidia_uvm: module uses symbols nvUvmInterfaceDisableAccessCntr from proprietary module nvidia, inheriting taint.
[  +0.059026] skl_hda_dsp_generic skl_hda_dsp_generic: ASoC: Parent card not yet available, widget card binding deferred
[  +0.059101] skl_hda_dsp_generic skl_hda_dsp_generic: hda_dsp_hdmi_build_controls: no PCM in topology for HDMI converter 3
[  +0.600659] kauditd_printk_skb: 49 callbacks suppressed
[ +23.117655] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000114] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000060] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000057] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.016911] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000077] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000032] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000031] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.016967] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000115] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000067] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[  +0.000070] [drm:nv_drm_master_set [nvidia_drm]] *ERROR* [nvidia-drm] [GPU ID 0x00000100] Failed to grab modeset ownership
[ +29.100658] kauditd_printk_skb: 27 callbacks suppressed
[May15 10:14] ACPI Error: No handler for Region [CMS0] (000000004b9a9d35) [SystemCMOS] (20230628/evregion-131)
[  +0.000017] ACPI Error: Region SystemCMOS (ID=5) has no handler (20230628/exfldio-261)

[  +0.000012] No Local Variables are initialized for Method [_Q33]

[  +0.000002] No Arguments are initialized for method [_Q33]

[  +0.000004] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0._Q33 due to previous error (AE_NOT_EXIST) (20230628/psparse-529)
[May15 10:24] kauditd_printk_skb: 1 callbacks suppressed


==> etcd [988de64cbc18] <==
{"level":"warn","ts":"2025-05-15T10:31:20.705598Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-05-15T10:31:20.705669Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-05-15T10:31:20.705734Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-05-15T10:31:20.705745Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-05-15T10:31:20.705773Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-05-15T10:31:20.706239Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-05-15T10:31:20.706327Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":20,"max-cpu-available":20,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-05-15T10:31:20.707722Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.187017ms"}
{"level":"info","ts":"2025-05-15T10:31:20.709645Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-05-15T10:31:20.709687Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-05-15T10:31:20.709703Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-05-15T10:31:20.709716Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-05-15T10:31:20.709720Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-05-15T10:31:20.709747Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-05-15T10:31:20.711574Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-05-15T10:31:20.712437Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-05-15T10:31:20.713166Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-05-15T10:31:20.714553Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-05-15T10:31:20.714589Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-05-15T10:31:20.714716Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-15T10:31:20.714789Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-15T10:31:20.714783Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-15T10:31:20.714795Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-15T10:31:20.715062Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-05-15T10:31:20.715527Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-05-15T10:31:20.716640Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-05-15T10:31:20.716811Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-05-15T10:31:20.716827Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-05-15T10:31:20.716862Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-15T10:31:20.716867Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-15T10:31:20.910106Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-05-15T10:31:20.910157Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-05-15T10:31:20.910168Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-05-15T10:31:20.910180Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-05-15T10:31:20.910184Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-15T10:31:20.910189Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-05-15T10:31:20.910193Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-15T10:31:20.912434Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-15T10:31:20.912904Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-05-15T10:31:20.912915Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-15T10:31:20.912909Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-15T10:31:20.913047Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-15T10:31:20.913076Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-05-15T10:31:20.913106Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-05-15T10:31:20.913112Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-15T10:31:20.913139Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-15T10:31:20.913488Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-15T10:31:20.913521Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-15T10:31:20.913904Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-05-15T10:31:20.914026Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-05-15T10:41:20.962567Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":754}
{"level":"info","ts":"2025-05-15T10:41:20.966547Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":754,"took":"3.571914ms","hash":3998979534,"current-db-size-bytes":1818624,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1818624,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-05-15T10:41:20.966587Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3998979534,"revision":754,"compact-revision":-1}


==> kernel <==
 10:44:41 up 33 min,  0 users,  load average: 0.45, 0.55, 0.54
Linux minikube 6.8.0-59-generic #61~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr 15 17:03:15 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [b5f11c86b007] <==
I0515 10:31:21.352308       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0515 10:31:21.352351       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0515 10:31:21.352360       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0515 10:31:21.352363       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0515 10:31:21.352369       1 controller.go:78] Starting OpenAPI AggregationController
I0515 10:31:21.352355       1 controller.go:119] Starting legacy_token_tracking_controller
I0515 10:31:21.352378       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0515 10:31:21.352463       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0515 10:31:21.352576       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0515 10:31:21.352604       1 controller.go:142] Starting OpenAPI controller
I0515 10:31:21.352613       1 controller.go:90] Starting OpenAPI V3 controller
I0515 10:31:21.352622       1 naming_controller.go:294] Starting NamingConditionController
I0515 10:31:21.352628       1 establishing_controller.go:81] Starting EstablishingController
I0515 10:31:21.352633       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0515 10:31:21.352638       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0515 10:31:21.352643       1 crd_finalizer.go:269] Starting CRDFinalizer
I0515 10:31:21.355128       1 local_available_controller.go:156] Starting LocalAvailability controller
I0515 10:31:21.355141       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0515 10:31:21.355247       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0515 10:31:21.355254       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0515 10:31:21.362486       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0515 10:31:21.362500       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0515 10:31:21.362511       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0515 10:31:21.362525       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
E0515 10:31:21.428098       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0515 10:31:21.435978       1 shared_informer.go:320] Caches are synced for node_authorizer
I0515 10:31:21.441075       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0515 10:31:21.441121       1 policy_source.go:240] refreshing policies
I0515 10:31:21.452288       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0515 10:31:21.452314       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0515 10:31:21.452433       1 shared_informer.go:320] Caches are synced for configmaps
I0515 10:31:21.452584       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0515 10:31:21.452590       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0515 10:31:21.452654       1 aggregator.go:171] initial CRD sync complete...
I0515 10:31:21.452660       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0515 10:31:21.452669       1 autoregister_controller.go:144] Starting autoregister controller
I0515 10:31:21.452682       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0515 10:31:21.452689       1 cache.go:39] Caches are synced for autoregister controller
I0515 10:31:21.454262       1 controller.go:615] quota admission added evaluator for: namespaces
I0515 10:31:21.455179       1 cache.go:39] Caches are synced for LocalAvailability controller
I0515 10:31:21.455783       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0515 10:31:21.462738       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0515 10:31:21.631869       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0515 10:31:22.364792       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0515 10:31:22.367286       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0515 10:31:22.367294       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0515 10:31:22.659792       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0515 10:31:22.688924       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0515 10:31:22.760626       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0515 10:31:22.766215       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0515 10:31:22.767222       1 controller.go:615] quota admission added evaluator for: endpoints
I0515 10:31:22.770981       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0515 10:31:23.386113       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0515 10:31:23.595498       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0515 10:31:23.601246       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0515 10:31:23.607755       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0515 10:31:28.484198       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0515 10:31:28.886132       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0515 10:31:56.707832       1 alloc.go:330] "allocated clusterIPs" service="default/fastapi-service" clusterIPs={"IPv4":"10.98.130.116"}
I0515 10:31:56.713539       1 alloc.go:330] "allocated clusterIPs" service="default/redis-service" clusterIPs={"IPv4":"10.100.55.159"}


==> kube-controller-manager [ea69923b8dfa] <==
I0515 10:31:27.940188       1 shared_informer.go:320] Caches are synced for attach detach
I0515 10:31:27.940188       1 shared_informer.go:320] Caches are synced for crt configmap
I0515 10:31:27.940264       1 shared_informer.go:320] Caches are synced for cronjob
I0515 10:31:27.940318       1 shared_informer.go:320] Caches are synced for daemon sets
I0515 10:31:27.941901       1 shared_informer.go:320] Caches are synced for namespace
I0515 10:31:27.945521       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0515 10:31:27.945538       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0515 10:31:27.945557       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0515 10:31:27.945646       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0515 10:31:27.945648       1 shared_informer.go:320] Caches are synced for resource quota
I0515 10:31:27.957967       1 shared_informer.go:320] Caches are synced for garbage collector
I0515 10:31:29.044921       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="553.788354ms"
I0515 10:31:29.052113       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="7.154183ms"
I0515 10:31:29.052195       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="49.951µs"
I0515 10:31:29.053711       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="58.528µs"
I0515 10:31:30.439577       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="51.952µs"
I0515 10:31:33.727401       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0515 10:31:42.818076       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="11.654644ms"
I0515 10:31:42.824632       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="6.477017ms"
I0515 10:31:42.824855       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="147.356µs"
I0515 10:31:42.826112       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="165.341µs"
I0515 10:31:42.831567       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="93.859µs"
I0515 10:31:44.520384       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="115.128µs"
I0515 10:31:45.525683       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="115.501µs"
I0515 10:31:57.334442       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="59.025µs"
I0515 10:31:59.335290       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="68.756µs"
I0515 10:32:01.455699       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="5.566758ms"
I0515 10:32:01.455814       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="68.734µs"
I0515 10:32:11.337822       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="47.936µs"
I0515 10:32:14.332831       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="40.835µs"
I0515 10:32:24.330783       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="28.254µs"
I0515 10:32:28.333318       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="41.75µs"
I0515 10:32:36.334690       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="40.71µs"
I0515 10:32:41.336402       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="80.572µs"
I0515 10:32:51.336701       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="49.299µs"
I0515 10:32:52.334331       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="80.274µs"
I0515 10:33:29.335696       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="61.666µs"
I0515 10:33:30.334386       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="30.77µs"
I0515 10:33:43.336385       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="76.315µs"
I0515 10:33:45.336608       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="55.281µs"
I0515 10:34:50.339508       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="55.897µs"
I0515 10:34:58.338507       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="45.362µs"
I0515 10:35:04.341468       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="52.173µs"
I0515 10:35:09.335987       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="41.918µs"
I0515 10:37:32.335209       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="61.79µs"
I0515 10:37:47.335816       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="39.412µs"
I0515 10:37:49.335863       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="74.989µs"
I0515 10:38:01.730318       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0515 10:38:04.340211       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="70.452µs"
I0515 10:42:34.891870       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-698496554f" duration="12.847049ms"
I0515 10:42:34.894838       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-698496554f" duration="2.913726ms"
I0515 10:42:34.894880       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-698496554f" duration="19.061µs"
I0515 10:42:34.896528       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-698496554f" duration="20.735µs"
I0515 10:42:37.953415       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-698496554f" duration="4.888261ms"
I0515 10:42:37.953493       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-698496554f" duration="42.537µs"
I0515 10:42:39.338797       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="74.4µs"
I0515 10:42:53.341362       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="99.76µs"
I0515 10:42:57.600164       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0515 10:43:01.335815       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="40.094µs"
I0515 10:43:14.334822       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-app-d94d949fd" duration="41.052µs"


==> kube-proxy [34693989e8b6] <==
I0515 10:31:29.417585       1 server_linux.go:66] "Using iptables proxy"
I0515 10:31:29.515833       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0515 10:31:29.515882       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0515 10:31:29.529596       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0515 10:31:29.529631       1 server_linux.go:170] "Using iptables Proxier"
I0515 10:31:29.531089       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0515 10:31:29.531362       1 server.go:497] "Version info" version="v1.32.0"
I0515 10:31:29.531375       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0515 10:31:29.532411       1 config.go:199] "Starting service config controller"
I0515 10:31:29.532413       1 config.go:105] "Starting endpoint slice config controller"
I0515 10:31:29.532433       1 shared_informer.go:313] Waiting for caches to sync for service config
I0515 10:31:29.532434       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0515 10:31:29.532441       1 config.go:329] "Starting node config controller"
I0515 10:31:29.532452       1 shared_informer.go:313] Waiting for caches to sync for node config
I0515 10:31:29.633280       1 shared_informer.go:320] Caches are synced for service config
I0515 10:31:29.633299       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0515 10:31:29.633327       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [e4546d100eb4] <==
I0515 10:31:20.951855       1 serving.go:386] Generated self-signed cert in-memory
W0515 10:31:21.376380       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0515 10:31:21.376419       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0515 10:31:21.376434       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0515 10:31:21.376446       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0515 10:31:21.388407       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0515 10:31:21.388420       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0515 10:31:21.389876       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0515 10:31:21.389904       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0515 10:31:21.389992       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0515 10:31:21.390034       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0515 10:31:21.390751       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0515 10:31:21.390777       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0515 10:31:21.391521       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0515 10:31:21.391520       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0515 10:31:21.391540       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0515 10:31:21.391549       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0515 10:31:21.391659       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0515 10:31:21.391688       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0515 10:31:21.391719       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0515 10:31:21.391732       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0515 10:31:21.391774       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0515 10:31:21.391783       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0515 10:31:21.392042       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0515 10:31:21.392052       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0515 10:31:21.392084       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0515 10:31:21.392091       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0515 10:31:21.392092       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0515 10:31:21.392101       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0515 10:31:21.392110       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
W0515 10:31:21.392115       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0515 10:31:21.392112       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0515 10:31:21.392109       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0515 10:31:21.392134       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0515 10:31:21.392140       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0515 10:31:21.392078       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0515 10:31:21.392158       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0515 10:31:21.392127       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0515 10:31:21.392174       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0515 10:31:21.392128       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0515 10:31:21.392183       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0515 10:31:21.392196       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0515 10:31:21.392216       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0515 10:31:22.251470       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0515 10:31:22.251531       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0515 10:31:22.358041       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0515 10:31:22.358086       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0515 10:31:22.371122       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0515 10:31:22.371148       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0515 10:31:22.394775       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0515 10:31:22.394799       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0515 10:31:22.405702       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0515 10:31:22.405729       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0515 10:31:22.411534       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0515 10:31:22.411560       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0515 10:31:22.447465       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0515 10:31:22.447492       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0515 10:31:22.512059       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0515 10:31:22.512088       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0515 10:31:24.090312       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 15 10:39:15 minikube kubelet[2473]: E0515 10:39:15.326094    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:39:19 minikube kubelet[2473]: E0515 10:39:19.324489    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:39:30 minikube kubelet[2473]: E0515 10:39:30.324737    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:39:30 minikube kubelet[2473]: E0515 10:39:30.324756    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:39:42 minikube kubelet[2473]: E0515 10:39:42.324914    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:39:45 minikube kubelet[2473]: E0515 10:39:45.324694    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:39:56 minikube kubelet[2473]: E0515 10:39:56.325584    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:39:56 minikube kubelet[2473]: E0515 10:39:56.325723    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:40:09 minikube kubelet[2473]: E0515 10:40:09.325779    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:40:10 minikube kubelet[2473]: E0515 10:40:10.325734    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:40:22 minikube kubelet[2473]: E0515 10:40:22.325014    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:40:23 minikube kubelet[2473]: E0515 10:40:23.332473    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:40:36 minikube kubelet[2473]: E0515 10:40:36.324423    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:40:37 minikube kubelet[2473]: E0515 10:40:37.331569    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:40:47 minikube kubelet[2473]: E0515 10:40:47.325327    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:40:48 minikube kubelet[2473]: E0515 10:40:48.324879    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:41:01 minikube kubelet[2473]: E0515 10:41:01.324832    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:41:02 minikube kubelet[2473]: E0515 10:41:02.325151    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:41:14 minikube kubelet[2473]: E0515 10:41:14.325289    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:41:14 minikube kubelet[2473]: E0515 10:41:14.325475    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:41:26 minikube kubelet[2473]: E0515 10:41:26.324327    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:41:29 minikube kubelet[2473]: E0515 10:41:29.330459    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:41:38 minikube kubelet[2473]: E0515 10:41:38.324627    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:41:42 minikube kubelet[2473]: E0515 10:41:42.325066    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:41:49 minikube kubelet[2473]: E0515 10:41:49.325723    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:41:55 minikube kubelet[2473]: E0515 10:41:55.334313    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:42:00 minikube kubelet[2473]: E0515 10:42:00.324710    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:42:07 minikube kubelet[2473]: E0515 10:42:07.325285    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:42:14 minikube kubelet[2473]: E0515 10:42:14.325569    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:42:19 minikube kubelet[2473]: E0515 10:42:19.324749    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:42:26 minikube kubelet[2473]: E0515 10:42:26.143811    2473 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-app:latest"
May 15 10:42:26 minikube kubelet[2473]: E0515 10:42:26.143909    2473 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-app:latest"
May 15 10:42:26 minikube kubelet[2473]: E0515 10:42:26.144197    2473 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:fastapi,Image:fastapi-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:CELERY_BROKER_URL,Value:redis://redis-service:6379/0,ValueFrom:nil,},EnvVar{Name:CELERY_RESULT_BACKEND,Value:redis://redis-service:6379/0,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rndq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fastapi-app-d94d949fd-cc4w8_default(118f0c39-a884-47b2-b095-59d82a3e8bba): ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 15 10:42:26 minikube kubelet[2473]: E0515 10:42:26.145601    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:42:32 minikube kubelet[2473]: E0515 10:42:32.325159    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:42:34 minikube kubelet[2473]: I0515 10:42:34.895701    2473 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5hcsg\" (UniqueName: \"kubernetes.io/projected/41374fd6-ba84-495d-bd74-9d91b409907b-kube-api-access-5hcsg\") pod \"redis-698496554f-8f62x\" (UID: \"41374fd6-ba84-495d-bd74-9d91b409907b\") " pod="default/redis-698496554f-8f62x"
May 15 10:42:37 minikube kubelet[2473]: I0515 10:42:37.948579    2473 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/redis-698496554f-8f62x" podStartSLOduration=1.9556603849999998 podStartE2EDuration="3.948553538s" podCreationTimestamp="2025-05-15 10:42:34 +0000 UTC" firstStartedPulling="2025-05-15 10:42:35.363495859 +0000 UTC m=+672.090679880" lastFinishedPulling="2025-05-15 10:42:37.356389005 +0000 UTC m=+674.083573033" observedRunningTime="2025-05-15 10:42:37.948345539 +0000 UTC m=+674.675529591" watchObservedRunningTime="2025-05-15 10:42:37.948553538 +0000 UTC m=+674.675737582"
May 15 10:42:39 minikube kubelet[2473]: E0515 10:42:39.324769    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:42:46 minikube kubelet[2473]: E0515 10:42:46.091993    2473 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-app:latest"
May 15 10:42:46 minikube kubelet[2473]: E0515 10:42:46.092059    2473 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-app:latest"
May 15 10:42:46 minikube kubelet[2473]: E0515 10:42:46.092215    2473 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:fastapi,Image:fastapi-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:CELERY_BROKER_URL,Value:redis://redis-service:6379/0,ValueFrom:nil,},EnvVar{Name:CELERY_RESULT_BACKEND,Value:redis://redis-service:6379/0,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-chmlm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fastapi-app-d94d949fd-5q2q6_default(04dc44e0-5dd7-450f-ab0b-9180c9b79794): ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 15 10:42:46 minikube kubelet[2473]: E0515 10:42:46.093517    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:42:53 minikube kubelet[2473]: E0515 10:42:53.325876    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:43:01 minikube kubelet[2473]: E0515 10:43:01.325341    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:43:08 minikube kubelet[2473]: E0515 10:43:08.325609    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:43:14 minikube kubelet[2473]: E0515 10:43:14.324977    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:43:19 minikube kubelet[2473]: E0515 10:43:19.325731    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:43:26 minikube kubelet[2473]: E0515 10:43:26.325472    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:43:31 minikube kubelet[2473]: E0515 10:43:31.324868    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:43:38 minikube kubelet[2473]: E0515 10:43:38.325478    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:43:44 minikube kubelet[2473]: E0515 10:43:44.325243    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:43:50 minikube kubelet[2473]: E0515 10:43:50.324638    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:43:59 minikube kubelet[2473]: E0515 10:43:59.324719    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:44:04 minikube kubelet[2473]: E0515 10:44:04.324634    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:44:12 minikube kubelet[2473]: E0515 10:44:12.325081    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:44:15 minikube kubelet[2473]: E0515 10:44:15.325501    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:44:27 minikube kubelet[2473]: E0515 10:44:27.325192    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:44:28 minikube kubelet[2473]: E0515 10:44:28.324679    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"
May 15 10:44:38 minikube kubelet[2473]: E0515 10:44:38.324961    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-cc4w8" podUID="118f0c39-a884-47b2-b095-59d82a3e8bba"
May 15 10:44:41 minikube kubelet[2473]: E0515 10:44:41.325777    2473 pod_workers.go:1301] "Error syncing pod, skipping" err="[failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", failed to \"StartContainer\" for \"celery\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"]" pod="default/fastapi-app-d94d949fd-5q2q6" podUID="04dc44e0-5dd7-450f-ab0b-9180c9b79794"


==> storage-provisioner [9d8d92c8b6e3] <==
I0515 10:31:29.881599       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0515 10:31:29.886078       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0515 10:31:29.886101       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0515 10:31:29.890869       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0515 10:31:29.890986       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_6e0cb39f-25a1-4db3-aa74-66c4c06677e8!
I0515 10:31:29.890997       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"05439c4e-8ee5-42f9-b844-9bda4117229d", APIVersion:"v1", ResourceVersion:"350", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_6e0cb39f-25a1-4db3-aa74-66c4c06677e8 became leader
I0515 10:31:29.991452       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_6e0cb39f-25a1-4db3-aa74-66c4c06677e8!

